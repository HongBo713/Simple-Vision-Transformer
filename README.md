# Vision Transformer (ViT) Implementation

## Overview

This project is an implementation of the Vision Transformer (ViT), which leverages the self-attention mechanism to capture intricate relationships between image patches. The original implementation was developed by Tin Nguyen and can be found [here](https://github.com/tintn/vision-transformer-from-scratch). This version includes additional visualization features and simplified parts for educational purposes.

## Features

- **Self-Attention Mechanism**: Dynamically focuses on different parts of the input image.
- **Multi-Head Attention**: Captures features at different levels.
- **Visualization**: Visualizes the attention scores to understand how the model processes the image patches.

## Installation

1. Clone the repository:
   ```bash
   git clone https://github.com/your-username/Simple-Vision-Transformer.git
   cd Simple-Vision-Transformer
   
## Acknowledgements
- Original implementation by by Tin Nguyen and can be found [here](https://github.com/tintn/vision-transformer-from-scratch).
- Inspired by the work [AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE](https://arxiv.org/pdf/2010.11929).

## Contributing
Contributions are welcome! Please submit a pull request or open an issue for any improvements or bug fixes.
